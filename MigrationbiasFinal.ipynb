{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c2ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ade5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token='AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM'\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d4f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Collects data from Twitter Academic API endpoints and provides them as json or csv files. Requires a bearer token for authentication.\n",
    "    '''\n",
    "    def __init__(self, bearer_token):\n",
    "        self.bearer_token = bearer_token\n",
    "        self.headers = {\"Authorization\": \"Bearer {}\".format(self.bearer_token)}\n",
    "        \n",
    "        '''\n",
    "        Initialize query parameters for the Twitter Full Archive Search endpoint.\n",
    "        Args:\n",
    "            start_date (str): the start time of the period. It needs to be a valid timestamp.\n",
    "            end_data (str): the end time of the period. It needs to be a valid timestamp.\n",
    "            keyword (str): the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            max_results (int): maximum number of tweets to retrieve for the desired period\n",
    "        Returns:\n",
    "            query_params (dict): dictionary containing all parameters for the Archive Search query.            \n",
    "        '''\n",
    "        \n",
    "    def create_tweet_query(self,\n",
    "                     start_date='2020-01-01 T00:00:00.000Z', \n",
    "                     end_date='2020-02-01 T00:00:00.000Z',\n",
    "                     keyword=\"place_country:BE has:geo lang:nl\",\n",
    "                     max_results = 500):\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id,referenced_tweets.id',\n",
    "                        'tweet.fields': 'id,text,author_id,context_annotations,geo,created_at,lang,public_metrics,entities,reply_settings,possibly_sensitive,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,location,public_metrics,verified,entities,profile_image_url',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return query_params\n",
    "    \n",
    "    def create_follower_query(self, screen_name, cursor=None):\n",
    "            '''\n",
    "            Initialize query parameters for the Twitter followers ids API endpoint.\n",
    "            Args:\n",
    "                screen_name (str): screenname of the user for which the followers ids need to be retrieved.\n",
    "                cursor (int): cursor to the next batch of followers ids.\n",
    "            Returns:\n",
    "                query_params (dict): dictionary containing all parameters for the Archive Search query.            \n",
    "            '''\n",
    "            query_params = {'screen_name': screen_name, 'cursor': cursor}\n",
    "            return query_params\n",
    "        \n",
    "    def connect_to_endpoint(self, search_api, query_params, next_token = None):\n",
    "        '''\n",
    "        Establish connection with the Twitter API endpoint.\n",
    "        '''\n",
    "        query_params['next_token'] = next_token   \n",
    "        response = requests.request(\"GET\", search_api, headers = self.headers, params = query_params)\n",
    "        print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "        return response.json()\n",
    "    \n",
    "    def retrieve_tweet(self, start_list, end_list, keyword=\"place_country:BE has:geo lang:nl\", tweet_per_period = 10000):\n",
    "        '''\n",
    "        Retrieve tweets from the Full Archive Search, following a specific query, for a given number of tweets per time periods.\n",
    "        Args:\n",
    "            start_list (list): list containing the start times for each time period. Needs to have the same length as end_list.\n",
    "            end_list (list): list containing the end times for each time period. Needs to have the same length as start_list.\n",
    "            keyword (str):  the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            tweet_per_period (int): the maximum number of tweets to retrieve per time period.\n",
    "        Returns:\n",
    "            tweet_list (list): list containing all the Twitter response objects.\n",
    "        '''\n",
    "        tweets_list = []  \n",
    "        #Define the search API : Twitter Full Archive Search\n",
    "        search_api = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "        #Total number of tweets we collected from the loop\n",
    "        total_tweets = 0\n",
    "        #Ensures that we collect less than or equal to the desired number of tweets\n",
    "        #Define the number of results per query. The API accepts a maximum of 500 tweets per query.\n",
    "        if tweet_per_period >= 500:\n",
    "            max_results=500\n",
    "        else:\n",
    "            max_results=tweet_per_period\n",
    "        for i in range(0,len(start_list)):\n",
    "            count = 0 # Counting tweets per time period\n",
    "            #Max tweets per period is set a bit lower than the real objective\n",
    "            flag = True\n",
    "            next_token = None\n",
    "       \n",
    "            while flag:\n",
    "                # Check if max_count reached\n",
    "                if count >= tweet_per_period:\n",
    "                    break\n",
    "                print(\"-------------------\")\n",
    "                print(\"Token: \", next_token)\n",
    "                query = self.create_tweet_query(start_list[i],end_list[i],keyword,max_results)\n",
    "                json_response = self.connect_to_endpoint(search_api, query, next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                    # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count != None and result_count > 0 and next_token != None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count != None and result_count > 0:\n",
    "                        print(\"-------------------\")\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                    #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                time.sleep(3)\n",
    "        print(\"Total number of results: \", total_tweets)\n",
    "        \n",
    "        return tweets_list\n",
    "    \n",
    "    def retrieve_all_followers(self,screen_name,sleep=60):\n",
    "        '''\n",
    "        Retrieve all the followers of a given user.\n",
    "        Args:\n",
    "            screen_name (str): the screenname of the user.\n",
    "            sleep (int): waiting time between two queries to avoid exceeding the API call limits.\n",
    "        Returns:\n",
    "            followers (list): list of all followers ids.\n",
    "        '''\n",
    "        search_api = \"https://api.twitter.com/1.1/followers/ids.json\"\n",
    "        followers = []\n",
    "        flag = True\n",
    "        next_cursor = None\n",
    "        count = 0\n",
    "        while flag:\n",
    "            print(\"-------------------\")\n",
    "            print(\"Cursor: \", next_cursor)\n",
    "            query = self.create_follower_query(screen_name,next_cursor)\n",
    "            json_response = self.connect_to_endpoint(search_api,query, next_cursor)\n",
    "            result_count = len(json_response['ids'])\n",
    "\n",
    "            if json_response['next_cursor'] != 0 :\n",
    "                # Save the cursor to use for next call\n",
    "                next_cursor = json_response['next_cursor']\n",
    "                print(\"Next cursor: \", next_cursor)\n",
    "                if result_count != None and result_count > 0 and next_cursor != 0:\n",
    "                    followers += json_response[\"ids\"] #concatenate list\n",
    "                    count += result_count\n",
    "                    print(\"Total # of followers added : %s\"%result_count)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(sleep)\n",
    "            # If no next cursor exists\n",
    "            else:\n",
    "                if result_count != None and result_count > 0:\n",
    "                    print(\"-------------------\")\n",
    "                    followers += json_response[\"ids\"] \n",
    "                    count += result_count\n",
    "                    print(\"Total # of followers added : %s\"%result_count)\n",
    "                    print(\"-------------------\")\n",
    "                    time.sleep(sleep)\n",
    "\n",
    "                #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                flag = False\n",
    "            \n",
    "        print(\"Total number of results: \", count)\n",
    "        return followers\n",
    "    def save_json(self, tweets, output_path='data/raw_tweets/tweet_dataset.json'):\n",
    "        \n",
    "        '''\n",
    "        Save the Twitter response as a json file.\n",
    "        '''\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(tweets,f)\n",
    "\n",
    "    def load_json(self, file_path='data/raw_tweets/tweet_dataset.json'):\n",
    "        \n",
    "        '''\n",
    "        Load a Twitter response json file.\n",
    "        '''\n",
    "        with open(file_path) as json_file:\n",
    "            return json.load(json_file)\n",
    "   \n",
    "\n",
    "    def to_dataframe(self, tweets):\n",
    "        \n",
    "        '''\n",
    "        Convert the json raw data to a tabular Pandas DataFrame.\n",
    "        Args:\n",
    "            data (json):raw data collected from the Twitter API\n",
    "        Returns:\n",
    "            tweet_df (pd.DataFrame) : DataFrame with tweet text and metadata\n",
    "            location_df (pd.DataFrame) : DataFrame with tweet geolocation data\n",
    "            user_df (pd.DataFrame) : DataFrame with user metadata\n",
    "       '''\n",
    "        \n",
    "    #tweet\n",
    "        tweet_id = []\n",
    "        user_id_t  = []\n",
    "        location_id_t = []\n",
    "        created_at_t = []\n",
    "        language =  []\n",
    "        source = []\n",
    "        retweet_count = []\n",
    "        quote_count = []\n",
    "        like_count = []\n",
    "        reply_count = []\n",
    "        possibly_sensitive =  []\n",
    "        reply_settings = []\n",
    "        text = []\n",
    "        tweet_mentions= []\n",
    "\n",
    "        #location\n",
    "        location_id_l =  []\n",
    "        country = []\n",
    "        place_type = []\n",
    "        name_l = []\n",
    "        longitude = []\n",
    "        latitude = []\n",
    "\n",
    "        #user\n",
    "        user_id_u  = []\n",
    "        created_at_u = []\n",
    "        name_u = []\n",
    "        screen_name = []\n",
    "        description = []\n",
    "        verified  = []\n",
    "        profile_image_URL = []\n",
    "        user_location = []\n",
    "        profile_mentions = []\n",
    "        followers_count = []\n",
    "        following_count = []\n",
    "        tweet_count = []\n",
    "        listed_count = []\n",
    "\n",
    "        for i in tqdm(range(len(tweets))):\n",
    "            #split content\n",
    "            tweet_content = tweets[i]['data']\n",
    "            tweet_location = tweets[i]['includes']['places']\n",
    "            user_profile = tweets[i]['includes']['users']\n",
    "            #tweet\n",
    "            for t in range(len(tweet_content)) : \n",
    "\n",
    "                tweet_id.append(tweet_content[t]['id'])\n",
    "                user_id_t.append(tweet_content[t]['author_id'])\n",
    "                if 'geo' in tweet_content[t].keys():\n",
    "                    location_id_t.append(tweet_content[t]['geo']['place_id'] )\n",
    "                else:\n",
    "                    location_id_t.append('No geotag')\n",
    "                created_at_t.append(dateutil.parser.parse(tweet_content[t]['created_at']))\n",
    "                language.append(tweet_content[t]['lang'] )\n",
    "                if 'source' in tweet_content[t].keys():\n",
    "                    source.append(tweet_content[t]['source'])\n",
    "                else:\n",
    "                    source.append('no source')\n",
    "                text.append(tweet_content[t]['text'] )\n",
    "                possibly_sensitive.append(tweet_content[t]['possibly_sensitive'])\n",
    "                reply_settings.append(tweet_content[t]['reply_settings'])\n",
    "                like_count.append(tweet_content[t]['public_metrics']['like_count'])\n",
    "                reply_count.append(tweet_content[t]['public_metrics']['reply_count'])\n",
    "                quote_count.append(tweet_content[t]['public_metrics']['quote_count'])\n",
    "                retweet_count.append(tweet_content[t]['public_metrics']['retweet_count'])       \n",
    "                mention_str = ''\n",
    "                if 'entities' in tweet_content[t].keys():\n",
    "                    if 'mentions' in tweet_content[t]['entities'].keys():\n",
    "                        mention_str += ''.join(str(tweet_content[t]['entities']['mentions'][e]['username']) +',' \n",
    "                                                for e in range(len(tweet_content[t]['entities']['mentions'])))\n",
    "                    else:\n",
    "                        mention_str += 'No mention,'\n",
    "                else:\n",
    "                    mention_str += 'No mention,'\n",
    "                \n",
    "                mention_str = mention_str[:-1]   #Remove the last comma\n",
    "\n",
    "                tweet_mentions.append(mention_str)\n",
    "                                \n",
    "                #location\n",
    "            for l in range(len(tweet_location)):\n",
    "                location_id_l.append(tweet_location[l]['id'])\n",
    "                country.append(tweet_location[l]['country'])\n",
    "                place_type.append(tweet_location[l]['place_type'])\n",
    "                name_l.append(tweet_location[l]['name'])\n",
    "                longitude.append((tweet_location[l]['geo']['bbox'][1]+tweet_location[l]['geo']['bbox'][3])/2)\n",
    "                latitude.append((tweet_location[l]['geo']['bbox'][0]+tweet_location[l]['geo']['bbox'][2])/2)\n",
    "\n",
    "                #user\n",
    "            for u in range(len(user_profile)):\n",
    "                user_id_u.append(user_profile[u][\"id\"] )\n",
    "                created_at_u.append(dateutil.parser.parse(user_profile[u]['created_at']))\n",
    "                name_u.append(user_profile[u][\"name\"] )\n",
    "                screen_name.append(user_profile[u][\"username\"])\n",
    "                description.append(user_profile[u][\"description\"] )\n",
    "                verified.append(user_profile[u][\"verified\"] )\n",
    "                profile_image_URL.append(user_profile[u][\"profile_image_url\"])\n",
    "                followers_count.append(user_profile[u][\"public_metrics\"][\"followers_count\"])\n",
    "                following_count.append(user_profile[u][\"public_metrics\"][\"following_count\"])\n",
    "                tweet_count.append(user_profile[u]['public_metrics']['tweet_count'])\n",
    "                listed_count.append(user_profile[u]['public_metrics']['listed_count'])\n",
    "                if \"location\" in user_profile[u].keys():\n",
    "                    user_location.append(user_profile[u][\"location\"])\n",
    "                else:\n",
    "                    user_location.append(None)\n",
    "                mentions_str = ''\n",
    "                if 'entities' in user_profile[u].keys():\n",
    "                    if 'description' in user_profile[u]['entities'].keys():\n",
    "                        if 'mentions' in user_profile[u]['entities']['description'].keys():\n",
    "                            mentions_str += ''.join(str(user_profile[u]['entities']['description']['mentions'][e]['username']) +','\n",
    "                                                        for e in range(len(user_profile[u]['entities']['description']['mentions'])))\n",
    "                        else : \n",
    "                            mentions_str += 'No mention,'\n",
    "                    else:\n",
    "                        mentions_str += 'No mention,'\n",
    "                else:\n",
    "                        mentions_str += 'No mention,'\n",
    "                        \n",
    "                mention_str = mention_str[:-1]\n",
    "                profile_mentions.append(mentions_str)\n",
    "              \n",
    "        #Create the DataFrames\n",
    "        tweet_df = pd.DataFrame({\"tweet_id\":tweet_id,\n",
    "                                \"user_id\":user_id_t,\n",
    "                                \"location_id\":location_id_t,\n",
    "                                \"created_at\": created_at_t,\n",
    "                                \"language\":language,\n",
    "                                \"tweet_mentions\":tweet_mentions,\n",
    "                                \"reply_settings\":reply_settings,\n",
    "                                'possibly_sensitive':possibly_sensitive,\n",
    "                                \"like_count\":like_count,\n",
    "                                \"retweet_count\":retweet_count,\n",
    "                                \"quote_count\":quote_count,\n",
    "                                \"reply_count\":reply_count,\n",
    "                                \"source\":source,\n",
    "                                \"text\":text})\n",
    "\n",
    "        location_df = pd.DataFrame({\"location_id\":location_id_l,\n",
    "                                \"country\":country,\n",
    "                                \"place_type\":place_type,\n",
    "                                \"location_geo\": name_l,\n",
    "                                \"longitude\":longitude,\n",
    "                                \"latitude\":latitude})\n",
    "\n",
    "        user_df = pd.DataFrame({\"user_id\":user_id_u,\n",
    "                                \"account_created_at\":created_at_u,\n",
    "                                \"name\":name_u,\n",
    "                                \"screen_name\":screen_name,\n",
    "                                \"description\":description,\n",
    "                                \"profile_image_url\":profile_image_URL,\n",
    "                                \"location_profile\":user_location,\n",
    "                                \"profile_mentions\": profile_mentions,\n",
    "                                \"followers_count\":followers_count,\n",
    "                                \"following_count\":following_count,\n",
    "                                \"listed_count\":listed_count,\n",
    "                                \"tweet_count\":tweet_count,\n",
    "                                \"verified\":verified})\n",
    "         #Remove all duplicates\n",
    "        user_df = user_df.drop_duplicates(subset=['user_id'], \n",
    "                                          keep='first', inplace=False, ignore_index=False)\n",
    "        location_df = location_df.drop_duplicates(subset= ['location_id'],\n",
    "                                                      keep='first', inplace=False, ignore_index=False)\n",
    "        #Merge the information from tweet and location dataframes\n",
    "        tweet_df = tweet_df.merge(location_df,on='location_id',how='left')\n",
    "        return tweet_df, user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75748468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2014-01-01T00:00:00.000Z\n",
      "Total # of Tweets added:  8\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw161ft8hhntsrxyi5reqc0zuy0ytml\n",
      "Start Date:  2014-02-01T00:00:00.000Z\n",
      "Total # of Tweets added:  18\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw164flha5bsasz10vnzvwo6ainpev1\n",
      "Start Date:  2014-03-01T00:00:00.000Z\n",
      "Total # of Tweets added:  28\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw167hw2goob7dtyu2n1zmgdqgclu9p\n",
      "Start Date:  2014-04-01T00:00:00.000Z\n",
      "Total # of Tweets added:  38\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw16de2nqrkg2tu3rzqgwu8wk9s7uyl\n",
      "Start Date:  2014-05-01T00:00:00.000Z\n",
      "Total # of Tweets added:  48\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw16gea02fztobqxnfozowbsld8degt\n",
      "Start Date:  2014-06-01T00:00:00.000Z\n",
      "Total # of Tweets added:  58\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw16gfixou1otwjeij8er2b9kqey2yl\n",
      "Start Date:  2014-07-01T00:00:00.000Z\n",
      "Total # of Tweets added:  68\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw1r2vnsecu03v5ymigu4ug2hw40vwd\n",
      "Start Date:  2014-08-01T00:00:00.000Z\n",
      "Total # of Tweets added:  78\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw1r5vfk3p0zsdl15umu96a113nakcd\n",
      "Start Date:  2014-09-01T00:00:00.000Z\n",
      "Total # of Tweets added:  88\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw1r8w1zx6oey0wcrux04pjqkb4rx4t\n",
      "Start Date:  2014-10-01T00:00:00.000Z\n",
      "Total # of Tweets added:  98\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw1rbw8x92hw3slls5xpveuub2jm5x9\n",
      "Start Date:  2014-11-01T00:00:00.000Z\n",
      "Total # of Tweets added:  108\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Next Token:  1jzu9lk96gu5npw1resne2xyirhgmsqlunqpe20g6c8t\n",
      "Start Date:  2014-12-01T00:00:00.000Z\n",
      "Total # of Tweets added:  118\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-01-01T00:00:00.000Z\n",
      "Total # of Tweets added:  120\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-03-01T00:00:00.000Z\n",
      "Total # of Tweets added:  124\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-04-01T00:00:00.000Z\n",
      "Total # of Tweets added:  127\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-05-01T00:00:00.000Z\n",
      "Total # of Tweets added:  130\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-06-01T00:00:00.000Z\n",
      "Total # of Tweets added:  135\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-07-01T00:00:00.000Z\n",
      "Total # of Tweets added:  138\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-08-01T00:00:00.000Z\n",
      "Total # of Tweets added:  143\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2015-09-01T00:00:00.000Z\n",
      "Total # of Tweets added:  144\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-05-01T00:00:00.000Z\n",
      "Total # of Tweets added:  145\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Total number of results:  145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 1146.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dl = DataLoader(bearer_token)   \n",
    "\n",
    "#Retrieve around 200 tweets, 100 on 2020/01/01 and 100 on 2020/01/02, written in English\n",
    "tweet_list = dl.retrieve_tweet(start_list=['2014-01-01T00:00:00.000Z','2014-02-01T00:00:00.000Z',\n",
    "                                          '2014-03-01T00:00:00.000Z','2014-04-01T00:00:00.000Z',\n",
    "                                          '2014-05-01T00:00:00.000Z','2014-06-01T00:00:00.000Z',\n",
    "                                          '2014-07-01T00:00:00.000Z','2014-08-01T00:00:00.000Z',\n",
    "                                          '2014-09-01T00:00:00.000Z','2014-10-01T00:00:00.000Z',\n",
    "                                          '2014-11-01T00:00:00.000Z','2014-12-01T00:00:00.000Z',\n",
    "                                           \n",
    "                                          '2015-01-01T00:00:00.000Z','2015-02-01T00:00:00.000Z',\n",
    "                                          '2015-03-01T00:00:00.000Z','2015-04-01T00:00:00.000Z',\n",
    "                                          '2015-05-01T00:00:00.000Z','2015-06-01T00:00:00.000Z',\n",
    "                                          '2015-07-01T00:00:00.000Z','2015-08-01T00:00:00.000Z',\n",
    "                                          '2015-09-01T00:00:00.000Z','2015-10-01T00:00:00.000Z',\n",
    "                                          '2015-11-01T00:00:00.000Z','2015-12-01T00:00:00.000Z',\n",
    "                                           \n",
    "                                          '2016-01-01T00:00:00.000Z','2016-02-01T00:00:00.000Z',\n",
    "                                          '2016-03-01T00:00:00.000Z','2016-04-01T00:00:00.000Z',\n",
    "                                          '2016-05-01T00:00:00.000Z','2016-06-01T00:00:00.000Z',\n",
    "                                          '2016-07-01T00:00:00.000Z','2016-08-01T00:00:00.000Z',\n",
    "                                          '2016-09-01T00:00:00.000Z','2016-10-01T00:00:00.000Z',\n",
    "                                          '2016-11-01T00:00:00.000Z','2016-12-01T00:00:00.000Z',\n",
    "                                           \n",
    "                                          '2017-01-01T00:00:00.000Z','2017-02-01T00:00:00.000Z',\n",
    "                                          '2017-03-01T00:00:00.000Z','2017-04-01T00:00:00.000Z',\n",
    "                                          '2017-05-01T00:00:00.000Z','2017-06-01T00:00:00.000Z',\n",
    "                                          '2017-07-01T00:00:00.000Z','2017-08-01T00:00:00.000Z',\n",
    "                                          '2017-09-01T00:00:00.000Z','2017-10-01T00:00:00.000Z',\n",
    "                                          '2017-11-01T00:00:00.000Z','2017-12-01T00:00:00.000Z',\n",
    "                                           \n",
    "                                          '2018-01-01T00:00:00.000Z','2018-02-01T00:00:00.000Z',\n",
    "                                          '2018-03-01T00:00:00.000Z','2018-04-01T00:00:00.000Z',\n",
    "                                          '2018-05-01T00:00:00.000Z','2018-06-01T00:00:00.000Z',\n",
    "                                          '2018-07-01T00:00:00.000Z','2018-08-01T00:00:00.000Z',\n",
    "                                          '2018-09-01T00:00:00.000Z','2018-10-01T00:00:00.000Z',\n",
    "                                          '2018-11-01T00:00:00.000Z','2018-12-01T00:00:00.000Z'],\n",
    "                               \n",
    "                               end_list=['2014-01-31T00:00:00.000Z','2014-02-28T00:00:00.000Z',\n",
    "                                        '2014-03-31T00:00:00.000Z','2014-04-30T00:00:00.000Z',\n",
    "                                        '2014-05-31T00:00:00.000Z','2014-06-30T00:00:00.000Z',\n",
    "                                        '2014-07-31T00:00:00.000Z','2014-08-31T00:00:00.000Z',\n",
    "                                        '2014-09-30T00:00:00.000Z','2014-10-31T00:00:00.000Z',\n",
    "                                        '2014-11-30T00:00:00.000Z','2014-12-31T00:00:00.000Z',\n",
    "                                        \n",
    "                                        '2015-01-31T00:00:00.000Z','2015-02-28T00:00:00.000Z',\n",
    "                                        '2015-03-31T00:00:00.000Z','2015-04-30T00:00:00.000Z',\n",
    "                                        '2015-05-31T00:00:00.000Z','2015-06-30T00:00:00.000Z',\n",
    "                                        '2015-07-31T00:00:00.000Z','2015-08-31T00:00:00.000Z',\n",
    "                                        '2015-09-30T00:00:00.000Z','2015-10-31T00:00:00.000Z',\n",
    "                                        '2015-11-30T00:00:00.000Z','2015-12-31T00:00:00.000Z',\n",
    "                                        \n",
    "                                        '2016-01-31T00:00:00.000Z','2016-02-29T00:00:00.000Z',\n",
    "                                        '2016-03-31T00:00:00.000Z','2016-04-30T00:00:00.000Z',\n",
    "                                        '2016-05-31T00:00:00.000Z','2016-06-30T00:00:00.000Z',\n",
    "                                        '2016-07-31T00:00:00.000Z','2016-08-31T00:00:00.000Z',\n",
    "                                        '2016-09-30T00:00:00.000Z','2016-10-31T00:00:00.000Z',\n",
    "                                        '2016-11-30T00:00:00.000Z','2016-12-31T00:00:00.000Z',\n",
    "                                        \n",
    "                                        '2017-01-31T00:00:00.000Z','2017-02-28T00:00:00.000Z',\n",
    "                                        '2017-03-31T00:00:00.000Z','2017-04-30T00:00:00.000Z',\n",
    "                                        '2017-05-31T00:00:00.000Z','2017-06-30T00:00:00.000Z',\n",
    "                                        '2017-07-31T00:00:00.000Z','2017-08-31T00:00:00.000Z',\n",
    "                                        '2017-09-30T00:00:00.000Z','2017-10-31T00:00:00.000Z',\n",
    "                                        '2017-11-30T00:00:00.000Z','2017-12-31T00:00:00.000Z',\n",
    "                                        \n",
    "                                        '2018-01-31T00:00:00.000Z','2018-02-28T00:00:00.000Z',\n",
    "                                        '2018-03-31T00:00:00.000Z','2018-04-30T00:00:00.000Z',\n",
    "                                        '2018-05-31T00:00:00.000Z','2018-06-30T00:00:00.000Z',\n",
    "                                        '2018-07-31T00:00:00.000Z','2018-08-31T00:00:00.000Z',\n",
    "                                        '2018-09-30T00:00:00.000Z','2018-10-31T00:00:00.000Z',\n",
    "                                        '2018-11-30T00:00:00.000Z','2018-12-31T00:00:00.000Z'],\n",
    "                                keyword= \"place_country:BE has:geo lang:nl from: 5799162\",\n",
    "                               tweet_per_period=10)\n",
    "                               \n",
    "#Convert the Twitter json output to csv  and save files                             \n",
    "tweet, user = dl.to_dataframe(tweet_list)\n",
    "geo_info = pd.DataFrame({\"datum\" : tweet[\"created_at\"], \"plaats\": tweet[\"location_geo\"]})\n",
    "data_lijst = geo_info[\"datum\"].tolist()\n",
    "plaats_lijst = geo_info[\"plaats\"].tolist()\n",
    "res = [[] for i in range(1,21)]\n",
    "for i in range(0,len(data_lijst)):\n",
    "        d = data_lijst[i]\n",
    "        jaar = d.year - 2010\n",
    "        maand = d.month\n",
    "        ip = (maand - 1)//3 + 1\n",
    "        p = 4*jaar - 15 \n",
    "        kolom = p + ip - 2\n",
    "        res[kolom].append(plaats_lijst[i])\n",
    "\n",
    "for i in range(0,20):\n",
    "    res[i].sort()\n",
    "    loc_lijst = list(set(res[i]))\n",
    "    extra = [(l,res[i].count(l)) for l in loc_lijst]\n",
    "    extra.sort(key = lambda x: x[1])\n",
    "    res[i].extend(extra)\n",
    "        \n",
    "g = max(len(res[i]) for i in range(0,20))\n",
    "for i in range(0,20):  \n",
    "    laatste = len(res[i])\n",
    "    v = g - len(res[i])\n",
    "    for j in range(0,v): \n",
    "        res[i].append(\" \") \n",
    "    res[i].append(\"FINAL\")\n",
    "    res[i].append(res[i][laatste-1][0].upper())\n",
    "    \n",
    "iks = {\"periode 1\" : res[0],\n",
    "       \"periode 2\" : res[1],\n",
    "       \"periode 3\" : res[2],\n",
    "       \"periode 4\" : res[3],\n",
    "       \"periode 5\" : res[4],\n",
    "       \"periode 6\" : res[5],\n",
    "       \"periode 7\" : res[6],\n",
    "       \"periode 8\" : res[7],\n",
    "       \"periode 9\" : res[8],\n",
    "       \"periode 10\" : res[9],\n",
    "       \"periode 11\" : res[10],\n",
    "       \"periode 12\" : res[11],\n",
    "       \"periode 13\" : res[12],\n",
    "       \"periode 14\" : res[13],\n",
    "       \"periode 15\" : res[14],\n",
    "       \"periode 16\" : res[15],\n",
    "       \"periode 17\" : res[16],\n",
    "       \"periode 18\" : res[17],\n",
    "       \"periode 19\" : res[18],\n",
    "       \"periode 20\" : res[19]\n",
    "      }\n",
    "\n",
    "    \n",
    "migration_info = pd.DataFrame(iks) \n",
    "    \n",
    "tweet.to_csv('C:\\\\Users\\\\nadeg\\\\OneDrive\\\\Documenten\\\\uitvoerbestanden\\\\tweet_output_path.csv')\n",
    "user.to_csv('C:\\\\Users\\\\nadeg\\\\OneDrive\\\\Documenten\\\\uitvoerbestanden\\\\user_output_path.csv')\n",
    "migration_info.to_csv('C:\\\\Users\\\\nadeg\\\\OneDrive\\\\Documenten\\\\uitvoerbestanden\\\\migration_output_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9fba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
