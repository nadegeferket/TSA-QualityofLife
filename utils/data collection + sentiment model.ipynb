{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d150d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nadeg\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\nadeg\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\nadeg\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26717f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token='AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM'\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62f3cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Collects data from Twitter Academic API endpoints and provides them as json or csv files. Requires a bearer token for authentication.\n",
    "    '''\n",
    "    def __init__(self, bearer_token):\n",
    "        self.bearer_token = bearer_token\n",
    "        self.headers = {\"Authorization\": \"Bearer {}\".format(self.bearer_token)}\n",
    "        \n",
    "        '''\n",
    "        Initialize query parameters for the Twitter Full Archive Search endpoint.\n",
    "        Args:\n",
    "            start_date (str): the start time of the period. It needs to be a valid timestamp.\n",
    "            end_data (str): the end time of the period. It needs to be a valid timestamp.\n",
    "            keyword (str): the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            max_results (int): maximum number of tweets to retrieve for the desired period\n",
    "        Returns:\n",
    "            query_params (dict): dictionary containing all parameters for the Archive Search query.            \n",
    "        '''\n",
    "        \n",
    "    def create_tweet_query(self,\n",
    "                     start_date='2020-01-01 T00:00:00.000Z', \n",
    "                     end_date='2020-02-01 T00:00:00.000Z',\n",
    "                     keyword=\"place_country:BE has:geo lang:nl\",\n",
    "                     max_results = 500):\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id,referenced_tweets.id',\n",
    "                        'tweet.fields': 'id,text,author_id,context_annotations,geo,created_at,lang,public_metrics,entities,reply_settings,possibly_sensitive,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,location,public_metrics,verified,entities,profile_image_url',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return query_params\n",
    "    \n",
    "        \n",
    "    def connect_to_endpoint(self, search_api, query_params, next_token = None):\n",
    "        '''\n",
    "        Establish connection with the Twitter API endpoint.\n",
    "        '''\n",
    "        query_params['next_token'] = next_token   \n",
    "        response = requests.request(\"GET\", search_api, headers = self.headers, params = query_params)\n",
    "        print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "        return response.json()\n",
    "    \n",
    "    def retrieve_tweet(self, start_list, end_list, keyword=\"place_country:BE has:geo lang:nl\", tweet_per_period = 10000):\n",
    "        '''\n",
    "        Retrieve tweets from the Full Archive Search, following a specific query, for a given number of tweets per time periods.\n",
    "        Args:\n",
    "            start_list (list): list containing the start times for each time period. Needs to have the same length as end_list.\n",
    "            end_list (list): list containing the end times for each time period. Needs to have the same length as start_list.\n",
    "            keyword (str):  the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            tweet_per_period (int): the maximum number of tweets to retrieve per time period.\n",
    "        Returns:\n",
    "            tweet_list (list): list containing all the Twitter response objects.\n",
    "        '''\n",
    "        tweets_list = []  \n",
    "        #Define the search API : Twitter Full Archive Search\n",
    "        search_api = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "        #Total number of tweets we collected from the loop\n",
    "        total_tweets = 0\n",
    "        #Ensures that we collect less than or equal to the desired number of tweets\n",
    "        #Define the number of results per query. The API accepts a maximum of 500 tweets per query.\n",
    "        if tweet_per_period >= 500:\n",
    "            max_results=500\n",
    "        else:\n",
    "            max_results=tweet_per_period\n",
    "        for i in range(0,len(start_list)):\n",
    "            count = 0 # Counting tweets per time period\n",
    "            #Max tweets per period is set a bit lower than the real objective\n",
    "            flag = True\n",
    "            next_token = None\n",
    "       \n",
    "            while flag:\n",
    "                # Check if max_count reached\n",
    "                if count >= tweet_per_period:\n",
    "                    break\n",
    "                print(\"-------------------\")\n",
    "                print(\"Token: \", next_token)\n",
    "                query = self.create_tweet_query(start_list[i],end_list[i],keyword,max_results)\n",
    "                json_response = self.connect_to_endpoint(search_api, query, next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                    # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count != None and result_count > 0 and next_token != None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count != None and result_count > 0:\n",
    "                        print(\"-------------------\")\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                    #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                time.sleep(3)\n",
    "        print(\"Total number of results: \", total_tweets)\n",
    "        \n",
    "        return tweets_list\n",
    "    \n",
    "    \n",
    "    def load_json(self, file_path='data/raw_tweets/tweet_dataset.json'):\n",
    "        \n",
    "        '''\n",
    "        Load a Twitter response json file.\n",
    "        '''\n",
    "        with open(file_path) as json_file:\n",
    "            return json.load(json_file)\n",
    "   \n",
    "\n",
    "    def get_info(self,tweets):\n",
    "        \n",
    "        '''\n",
    "        Convert the json raw data to a tabular Pandas DataFrame.\n",
    "        Args:\n",
    "            data (json):raw data collected from the Twitter API\n",
    "        Returns:\n",
    "            text_list: list of texts from tweets\n",
    "            tweet_df (pd.DataFrame) : DataFrame with tweet text and metadata\n",
    "       '''\n",
    "        \n",
    "    #tweet\n",
    "        \n",
    "        text = []\n",
    "\n",
    "        for i in tqdm(range(len(tweets))):\n",
    "            tweet_content = tweets[i]['data']\n",
    "            #tweet\n",
    "            for t in range(len(tweet_content)) : \n",
    "                \n",
    "                text.append(tweet_content[t]['text'] )\n",
    "        tweet_df = pd.DataFrame({\"text\":text})   \n",
    "        return text, tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea243597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2020-01-01T00:00:00.000Z\n",
      "Total # of Tweets added:  84\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2020-02-01T00:00:00.000Z\n",
      "Total # of Tweets added:  180\n",
      "-------------------\n",
      "Total number of results:  180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "Predicting...: 100%|██████████| 6/6 [00:34<00:00,  5.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEGATIVE']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twitter_sentiment_classifier import batch_predict\n",
    "dl = DataLoader(bearer_token)   \n",
    "\n",
    "#Retrieve around 200 tweets, 100 on 2020/01/01 and 100 on 2020/01/02, written in English\n",
    "tweet_list = dl.retrieve_tweet(start_list=['2020-01-01T00:00:00.000Z','2020-02-01T00:00:00.000Z'],\n",
    "                               end_list=['2020-01-31T00:00:00.000Z','2020-02-28T00:00:00.000Z'],\n",
    "                                keyword=\"overheid place_country:BE has:geo lang:nl\",\n",
    "                               tweet_per_period=100, \n",
    "                               )\n",
    "\n",
    "tekst_list, tekst_df = dl.get_info(tweet_list)\n",
    "tekst_df.to_csv('output_info.csv')\n",
    "for i in range(0,len(tekst_list)):\n",
    "    #preprocessing stap: vervangen van einde lijn naar spatie\n",
    "    tekst_list[i] = tekst_list[i].replace(\"\\n\",\" \")\n",
    "    \n",
    "batch_predict(tekst_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60befe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
