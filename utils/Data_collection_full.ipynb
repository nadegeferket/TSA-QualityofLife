{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8d150d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "26717f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token='AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM'\n",
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "62f3cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Collects data from Twitter Academic API endpoints and provides them as json or csv files. Requires a bearer token for authentication.\n",
    "    '''\n",
    "    def __init__(self, bearer_token):\n",
    "        self.bearer_token = bearer_token\n",
    "        self.headers = {\"Authorization\": \"Bearer {}\".format(self.bearer_token)}\n",
    "        \n",
    "        '''\n",
    "        Initialize query parameters for the Twitter Full Archive Search endpoint.\n",
    "        Args:\n",
    "            start_date (str): the start time of the period. It needs to be a valid timestamp.\n",
    "            end_data (str): the end time of the period. It needs to be a valid timestamp.\n",
    "            keyword (str): the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            max_results (int): maximum number of tweets to retrieve for the desired period\n",
    "        Returns:\n",
    "            query_params (dict): dictionary containing all parameters for the Archive Search query.            \n",
    "        '''\n",
    "        \n",
    "    def create_tweet_query(self,\n",
    "                     start_date='2020-01-01 T00:00:00.000Z', \n",
    "                     end_date='2020-02-01 T00:00:00.000Z',\n",
    "                     keyword=\"place_country:BE has:geo lang:nl\",\n",
    "                     max_results = 500):\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id,referenced_tweets.id',\n",
    "                        'tweet.fields': 'id,text,author_id,context_annotations,geo,created_at,lang,public_metrics,entities,reply_settings,possibly_sensitive,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,location,public_metrics,verified,entities,profile_image_url',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return query_params\n",
    "        \n",
    "    def connect_to_endpoint(self, search_api, query_params, next_token = None):\n",
    "        '''\n",
    "        Establish connection with the Twitter API endpoint.\n",
    "        '''\n",
    "        query_params['next_token'] = next_token   \n",
    "        response = requests.request(\"GET\", search_api, headers = self.headers, params = query_params)\n",
    "        print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "        return response.json()\n",
    "    \n",
    "    def retrieve_tweet(self, start_list, end_list, keyword, tweet_per_period = 10000):\n",
    "        '''\n",
    "        Retrieve tweets from the Full Archive Search, following a specific query, for a given number of tweets per time periods.\n",
    "        Args:\n",
    "            start_list (list): list containing the start times for each time period. Needs to have the same length as end_list.\n",
    "            end_list (list): list containing the end times for each time period. Needs to have the same length as start_list.\n",
    "            keyword (str):  the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            tweet_per_period (int): the maximum number of tweets to retrieve per time period.\n",
    "        Returns:\n",
    "            tweet_list (list): list containing all the Twitter response objects.\n",
    "        '''\n",
    "        tweets_list = []  \n",
    "        #Define the search API : Twitter Full Archive Search\n",
    "        search_api = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "        #Total number of tweets we collected from the loop\n",
    "        total_tweets = 0\n",
    "        #Ensures that we collect less than or equal to the desired number of tweets\n",
    "        #Define the number of results per query. The API accepts a maximum of 500 tweets per query.\n",
    "        if tweet_per_period >= 500:\n",
    "            max_results=500\n",
    "        else:\n",
    "            max_results=tweet_per_period\n",
    "        for i in range(0,len(start_list)):\n",
    "            count = 0 # Counting tweets per time period\n",
    "            #Max tweets per period is set a bit lower than the real objective\n",
    "            flag = True\n",
    "            next_token = None\n",
    "       \n",
    "            while flag:\n",
    "                # Check if max_count reached\n",
    "                if count >= tweet_per_period:\n",
    "                    break\n",
    "                print(\"-------------------\")\n",
    "                print(\"Token: \", next_token)\n",
    "                query = self.create_tweet_query(start_list[i],end_list[i],keyword,max_results)\n",
    "                json_response = self.connect_to_endpoint(search_api, query, next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                    # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count != None and result_count > 0 and next_token != None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count != None and result_count > 0:\n",
    "                        print(\"-------------------\")\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", total_tweets)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                    #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                time.sleep(3)\n",
    "        print(\"Total number of results: \", total_tweets)\n",
    "        \n",
    "        return tweets_list\n",
    "          \n",
    "    def save_json(self, tweets, output_path='data/raw_tweets/tweet_dataset.json'):\n",
    "        \n",
    "        '''\n",
    "        Save the Twitter response as a json file.\n",
    "        '''\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(tweets,f)\n",
    "\n",
    "    def load_json(self, file_path='data/raw_tweets/tweet_dataset.json'):\n",
    "        \n",
    "        '''\n",
    "        Load a Twitter response json file.\n",
    "        '''\n",
    "        with open(file_path) as json_file:\n",
    "            return json.load(json_file)\n",
    "   \n",
    "\n",
    "    def to_dataframe(self, tweets):\n",
    "        \n",
    "        '''\n",
    "        Convert the json raw data to a tabular Pandas DataFrame.\n",
    "        Args:\n",
    "            data (json):raw data collected from the Twitter API\n",
    "        Returns:\n",
    "            tweet_df (pd.DataFrame) : DataFrame with tweet text and metadata\n",
    "            location_df (pd.DataFrame) : DataFrame with tweet geolocation data\n",
    "            user_df (pd.DataFrame) : DataFrame with user metadata\n",
    "       '''\n",
    "        \n",
    "    #tweet\n",
    "        tweet_id = []\n",
    "        user_id_t  = []\n",
    "        location_id_t = []\n",
    "        created_at_t = []\n",
    "        language =  []\n",
    "        source = []\n",
    "        retweet_count = []\n",
    "        quote_count = []\n",
    "        like_count = []\n",
    "        reply_count = []\n",
    "        possibly_sensitive =  []\n",
    "        reply_settings = []\n",
    "        text = []\n",
    "        tweet_mentions= []\n",
    "\n",
    "        #location\n",
    "        location_id_l =  []\n",
    "        country = []\n",
    "        place_type = []\n",
    "        name_l = []\n",
    "        longitude = []\n",
    "        latitude = []\n",
    "\n",
    "        #user\n",
    "        user_id_u  = []\n",
    "        created_at_u = []\n",
    "        name_u = []\n",
    "        screen_name = []\n",
    "        description = []\n",
    "        verified  = []\n",
    "        profile_image_URL = []\n",
    "        user_location = []\n",
    "        profile_mentions = []\n",
    "        followers_count = []\n",
    "        following_count = []\n",
    "        tweet_count = []\n",
    "        listed_count = []\n",
    "\n",
    "        for i in tqdm(range(len(tweets))):\n",
    "            #split content\n",
    "            tweet_content = tweets[i]['data']\n",
    "            if 'places' in tweets[i]['includes'].keys():\n",
    "                tweet_location = tweets[i]['includes']['places']\n",
    "            else:\n",
    "                tweet_location = None\n",
    "                \n",
    "            user_profile = tweets[i]['includes']['users']\n",
    "            #tweet\n",
    "            for t in range(len(tweet_content)) : \n",
    "                tweet_id.append(tweet_content[t]['id'])\n",
    "                user_id_t.append(tweet_content[t]['author_id'])\n",
    "                if 'geo' in tweet_content[t].keys():\n",
    "                    location_id_t.append(tweet_content[t]['geo']['place_id'] )\n",
    "                else:\n",
    "                    location_id_t.append('No geotag')\n",
    "                created_at_t.append(dateutil.parser.parse(tweet_content[t]['created_at']))\n",
    "                language.append(tweet_content[t]['lang'] )\n",
    "                if 'source' in tweet_content[t].keys():\n",
    "                    source.append(tweet_content[t]['source'])\n",
    "                else:\n",
    "                    source.append('No source')\n",
    "                text.append(tweet_content[t]['text'] )\n",
    "                possibly_sensitive.append(tweet_content[t]['possibly_sensitive'])\n",
    "                reply_settings.append(tweet_content[t]['reply_settings'])\n",
    "                like_count.append(tweet_content[t]['public_metrics']['like_count'])\n",
    "                reply_count.append(tweet_content[t]['public_metrics']['reply_count'])\n",
    "                quote_count.append(tweet_content[t]['public_metrics']['quote_count'])\n",
    "                retweet_count.append(tweet_content[t]['public_metrics']['retweet_count'])       \n",
    "                mention_str = ''\n",
    "                if 'entities' in tweet_content[t].keys():\n",
    "                    if 'mentions' in tweet_content[t]['entities'].keys():\n",
    "                        mention_str += ''.join(str(tweet_content[t]['entities']['mentions'][e]['username']) +',' \n",
    "                                                for e in range(len(tweet_content[t]['entities']['mentions'])))\n",
    "                    else:\n",
    "                        mention_str += 'No mention,'\n",
    "                else:\n",
    "                    mention_str += 'No mention,'\n",
    "                \n",
    "                mention_str = mention_str[:-1]   #Remove the last comma\n",
    "\n",
    "                tweet_mentions.append(mention_str)\n",
    "                                \n",
    "                #location\n",
    "            if tweet_location != None:\n",
    "                for l in range(len(tweet_location)):\n",
    "                    location_id_l.append(tweet_location[l]['id'])\n",
    "                    country.append(tweet_location[l]['country'])\n",
    "                    place_type.append(tweet_location[l]['place_type'])\n",
    "                    name_l.append(tweet_location[l]['name'])\n",
    "                    longitude.append((tweet_location[l]['geo']['bbox'][1]+tweet_location[l]['geo']['bbox'][3])/2)\n",
    "                    latitude.append((tweet_location[l]['geo']['bbox'][0]+tweet_location[l]['geo']['bbox'][2])/2)\n",
    "\n",
    "                #user\n",
    "            for u in range(len(user_profile)):\n",
    "                user_id_u.append(user_profile[u][\"id\"] )\n",
    "                created_at_u.append(dateutil.parser.parse(user_profile[u]['created_at']))\n",
    "                name_u.append(user_profile[u][\"name\"] )\n",
    "                screen_name.append(user_profile[u][\"username\"])\n",
    "                description.append(user_profile[u][\"description\"] )\n",
    "                verified.append(user_profile[u][\"verified\"] )\n",
    "                profile_image_URL.append(user_profile[u][\"profile_image_url\"])\n",
    "                followers_count.append(user_profile[u][\"public_metrics\"][\"followers_count\"])\n",
    "                following_count.append(user_profile[u][\"public_metrics\"][\"following_count\"])\n",
    "                tweet_count.append(user_profile[u]['public_metrics']['tweet_count'])\n",
    "                listed_count.append(user_profile[u]['public_metrics']['listed_count'])\n",
    "                if \"location\" in user_profile[u].keys():\n",
    "                    user_location.append(user_profile[u][\"location\"])\n",
    "                else:\n",
    "                    user_location.append(None)\n",
    "                mentions_str = ''\n",
    "                if 'entities' in user_profile[u].keys():\n",
    "                    if 'description' in user_profile[u]['entities'].keys():\n",
    "                        if 'mentions' in user_profile[u]['entities']['description'].keys():\n",
    "                            mentions_str += ''.join(str(user_profile[u]['entities']['description']['mentions'][e]['username']) +','\n",
    "                                                        for e in range(len(user_profile[u]['entities']['description']['mentions'])))\n",
    "                        else : \n",
    "                            mentions_str += 'No mention,'\n",
    "                    else:\n",
    "                        mentions_str += 'No mention,'\n",
    "                else:\n",
    "                        mentions_str += 'No mention,'\n",
    "                        \n",
    "                mention_str = mention_str[:-1]\n",
    "                profile_mentions.append(mentions_str)\n",
    "              \n",
    "        #Create the DataFrames\n",
    "        tweet_df = pd.DataFrame({\"tweet_id\":tweet_id,\n",
    "                                \"user_id\":user_id_t,\n",
    "                                \"location_id\":location_id_t,\n",
    "                                \"created_at\": created_at_t,\n",
    "                                \"language\":language,\n",
    "                                \"source\": source,\n",
    "                                \"tweet_mentions\":tweet_mentions,\n",
    "                                \"reply_settings\":reply_settings,\n",
    "                                'possibly_sensitive':possibly_sensitive,\n",
    "                                \"like_count\":like_count,\n",
    "                                \"retweet_count\":retweet_count,\n",
    "                                \"quote_count\":quote_count,\n",
    "                                \"reply_count\":reply_count,    \n",
    "                                \"text\":text})\n",
    "\n",
    "        location_df = pd.DataFrame({\"location_id\":location_id_l,\n",
    "                                \"country\":country,\n",
    "                                \"place_type\":place_type,\n",
    "                                \"location_geo\": name_l,\n",
    "                                \"longitude\":longitude,\n",
    "                                \"latitude\":latitude})\n",
    "\n",
    "        user_df = pd.DataFrame({\"user_id\":user_id_u,\n",
    "                                \"account_created_at\":created_at_u,\n",
    "                                \"name\":name_u,\n",
    "                                \"screen_name\":screen_name,\n",
    "                                \"description\":description,\n",
    "                                \"profile_image_url\":profile_image_URL,\n",
    "                                \"location_profile\":user_location,\n",
    "                                \"profile_mentions\": profile_mentions,\n",
    "                                \"followers_count\":followers_count,\n",
    "                                \"following_count\":following_count,\n",
    "                                \"listed_count\":listed_count,\n",
    "                                \"tweet_count\":tweet_count,\n",
    "                                \"verified\":verified})\n",
    "         #Remove all duplicates\n",
    "        user_df = user_df.drop_duplicates(subset=['user_id'], \n",
    "                                          keep='first', inplace=False, ignore_index=False)\n",
    "        location_df = location_df.drop_duplicates(subset= ['location_id'],\n",
    "                                                      keep='first', inplace=False, ignore_index=False)\n",
    "        #Merge the information from tweet and location dataframes\n",
    "        tweet_df = tweet_df.merge(location_df,on='location_id',how='left')\n",
    "        return tweet_df, user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ea243597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-01-01T00:00:00.000Z\n",
      "Total # of Tweets added:  5\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-02-01T00:00:00.000Z\n",
      "Total # of Tweets added:  6\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-04-01T00:00:00.000Z\n",
      "Total # of Tweets added:  7\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-05-01T00:00:00.000Z\n",
      "Total # of Tweets added:  9\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-06-01T00:00:00.000Z\n",
      "Total # of Tweets added:  12\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-08-01T00:00:00.000Z\n",
      "Total # of Tweets added:  16\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-10-01T00:00:00.000Z\n",
      "Total # of Tweets added:  17\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Start Date:  2017-12-01T00:00:00.000Z\n",
      "Total # of Tweets added:  21\n",
      "-------------------\n",
      "Total number of results:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 272.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "dl = DataLoader(bearer_token)   \n",
    "\n",
    "#Retrieve around 200 tweets, 100 on 2020/01/01 and 100 on 2020/01/02, written in English\n",
    "tweet_list = dl.retrieve_tweet(start_list=['2017-01-01T00:00:00.000Z','2017-02-01T00:00:00.000Z',\n",
    "                                          '2017-03-01T00:00:00.000Z','2017-04-01T00:00:00.000Z',\n",
    "                                          '2017-05-01T00:00:00.000Z','2017-06-01T00:00:00.000Z',\n",
    "                                          '2017-07-01T00:00:00.000Z','2017-08-01T00:00:00.000Z',\n",
    "                                          '2017-09-01T00:00:00.000Z','2017-10-01T00:00:00.000Z',\n",
    "                                          '2017-11-01T00:00:00.000Z','2017-12-01T00:00:00.000Z'],\n",
    "                               end_list=['2017-01-31T00:00:00.000Z','2017-02-28T00:00:00.000Z',\n",
    "                                        '2017-03-31T00:00:00.000Z','2017-04-30T00:00:00.000Z',\n",
    "                                        '2017-05-31T00:00:00.000Z','2017-06-30T00:00:00.000Z',\n",
    "                                        '2017-07-31T00:00:00.000Z','2017-08-31T00:00:00.000Z',\n",
    "                                        '2017-09-30T00:00:00.000Z','2017-10-31T00:00:00.000Z',\n",
    "                                        '2017-11-30T00:00:00.000Z','2017-12-31T00:00:00.000Z'],\n",
    "                                keyword=\"mindfulness place_country:BE has:geo lang:nl\",\n",
    "                               tweet_per_period=100, \n",
    "                               )\n",
    "                               \n",
    "#Convert the Twitter json output to csv  and save files \n",
    "\n",
    "tweet, user = dl.to_dataframe(tweet_list)\n",
    "tweet.to_csv('tweet_info2017mindfulness.csv')\n",
    "user.to_csv('user_info2017mindfulness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "0112f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 172.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_provincie = pd.read_csv(\"Flemish_regions.csv\")\n",
    "provincieNamen = [\"Limburg\",\"Antwerpen\",\"Vlaams_Brabant\",\"West_Vlaanderen\",\"Oost_Vlaanderen\"]\n",
    "steden_per_provincie = [df_provincie[s] for s in provincieNamen]\n",
    "for i in tqdm(range(0,len(steden_per_provincie))):\n",
    "    rslt_df = tweet[tweet['location_geo'].isin(steden_per_provincie[i])]\n",
    "    rslt_df.to_csv(\"tweet_per_provincie_\" + provincieNamen[i] + \"famleden\" + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b8cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
