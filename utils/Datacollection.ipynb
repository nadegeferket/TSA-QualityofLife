'''
This data collection script is largely inspired from the excellent tutorial of Andrew Edward : 
https://towardsdatascience.com/an-extensive-guide-to-collecting-tweets-from-twitter-api-v2-for-academic-research-using-python-3-518fcb71df2a
'''
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427aee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import unicodedata\n",
    "\n",
    "bearer_token='   '\n",
    "os.environ['TOKEN'] = '   '\n",
    "\n",
    "'''\n",
    "    Initialize query parameters for the Twitter Full Archive Search endpoint.\n",
    "    Args:\n",
    "        start_date (str): the start time of the period. It needs to be a valid timestamp.\n",
    "        end_data (str): the end time of the period. It needs to be a valid timestamp.\n",
    "        keyword (str): the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "        max_results (int): maximum number of tweets to retrieve for the desired period\n",
    "    Returns:\n",
    "        query_params (dict): dictionary containing all parameters for the Archive Search query.            \n",
    "'''\n",
    "def create_tweet_query(start_date='2020-01-01 T00:00:00.000Z', \n",
    "                     end_date='2020-02-01 T00:00:00.000Z',\n",
    "                     keyword=\"place_country:BE has:geo lang:nl\",\n",
    "                     max_results = 500):\n",
    "        query_params = {'query': keyword,\n",
    "                        'start_time': start_date,\n",
    "                        'end_time': end_date,\n",
    "                        'max_results': max_results,\n",
    "                        'expansions': 'author_id,in_reply_to_user_id,geo.place_id,referenced_tweets.id',\n",
    "                        'tweet.fields': 'id,text,author_id,context_annotations,geo,created_at,lang,public_metrics,entities,reply_settings,possibly_sensitive,source',\n",
    "                        'user.fields': 'id,name,username,created_at,description,location,public_metrics,verified,entities,profile_image_url',\n",
    "                        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "                        'next_token': {}}\n",
    "        return query_params\n",
    "        \n",
    "\n",
    "class DataLoaderSimpleVersion:\n",
    "    '''\n",
    "    Collects data from Twitter Academic API endpoints and provides them as json or csv files. Requires a bearer token for authentication.\n",
    "    '''\n",
    "    def __init__(self, bearer_token):\n",
    "        self.headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "        \n",
    "    \n",
    "    def connect_to_endpoint(self,search_api, query_params, next_token = None):\n",
    "        '''\n",
    "        Establish connection with the Twitter API endpoint.\n",
    "        '''\n",
    "        query_params['next_token'] = next_token   \n",
    "        response = requests.request(\"GET\", search_api, headers = self.headers, params = query_params)\n",
    "        print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)\n",
    "        return response.json()\n",
    "    \n",
    "    \n",
    "    def retrieve_tweet(self, start_list, end_list, keyword=\"place_country:BE has:geo lang:nl\", tweet_per_period = 10000):\n",
    "        '''\n",
    "        Retrieve tweets from the Full Archive Search, following a specific query, for a given number of tweets per time periods.\n",
    "        Args:\n",
    "            start_list (list): list containing the start times for each time period. Needs to have the same length as end_list.\n",
    "            end_list (list): list containing the end times for each time period. Needs to have the same length as start_list.\n",
    "            keyword (str):  the query parameters to refine the tweet search. See the Twitter API documentation for more information on queries.\n",
    "            tweet_per_period (int): the maximum number of tweets to retrieve per time period.\n",
    "        Returns:\n",
    "            tweet_list (list): list containing all the Twitter response objects.\n",
    "        '''\n",
    "        tweets_list = []  \n",
    "        #Define the search API : Twitter Full Archive Search\n",
    "        search_api = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "        #Total number of tweets we collected from the loop\n",
    "        total_tweets = 0\n",
    "        #Ensures that we collect less than or equal to the desired number of tweets\n",
    "        #Define the number of results per query. The API accepts a maximum of 500 tweets per query.\n",
    "        max_results = min(tweet_per_period,500)\n",
    "            \n",
    "        for i in range(0,len(start_list)):\n",
    "            count = 0 # Counting tweets per time period\n",
    "            #Max tweets per period is set a bit lower than the real objective\n",
    "            flag = True\n",
    "            next_token = None\n",
    "            query = create_tweet_query(start_list[i],end_list[i],keyword,max_results)\n",
    "    \n",
    "            while flag and count <= tweet_per_period:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Token: \", next_token)\n",
    "                json_response = self.connect_to_endpoint(search_api, query, next_token)\n",
    "                result_count = json_response['meta']['result_count']\n",
    "\n",
    "                if 'next_token' in json_response['meta']:\n",
    "                    # Save the token to use for next call\n",
    "                    next_token = json_response['meta']['next_token']\n",
    "                    print(\"Next Token: \", next_token)\n",
    "                    if result_count != None and result_count > 0 and next_token != None:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", count)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                # If no next token exists\n",
    "                else:\n",
    "                    if result_count != None and result_count > 0:\n",
    "                        print(\"Start Date: \", start_list[i])\n",
    "                        tweets_list.append(json_response)\n",
    "                        count += result_count\n",
    "                        total_tweets += result_count\n",
    "                        print(\"Total # of Tweets added: \", count)\n",
    "                        print(\"-------------------\")\n",
    "                        time.sleep(3)\n",
    "\n",
    "                    #Since this is the final request, turn flag to false to move to the next time period.\n",
    "                    flag = False\n",
    "                    next_token = None\n",
    "                time.sleep(3)                \n",
    "            \n",
    "\n",
    "        print(\"Total number of results: \", total_tweets)\n",
    "        \n",
    "        return tweets_list\n",
    "  \n",
    "    def to_dataframe(self, tweets):\n",
    "        \n",
    "        '''\n",
    "        Convert the json raw data to a tabular Pandas DataFrame.\n",
    "        Args:\n",
    "            tweets (json):raw data collected from the Twitter API\n",
    "        Returns:\n",
    "            tweet_text_df (pd.DataFrame) : DataFrame with tweet text and some meta data\n",
    "\n",
    "       '''\n",
    "        \n",
    "    #tweet\n",
    "        location_id_t = []\n",
    "        tweet_id = []\n",
    "        text = []\n",
    "        \n",
    "    #location\n",
    "        location_id_l =  []\n",
    "        country = []\n",
    "        place_type = []\n",
    "        name_l = []\n",
    "\n",
    "\n",
    "        for i in tqdm(range(len(tweets))):\n",
    "            # get data from tweets[i] split content\n",
    "            tweet_content = tweets[i]['data']\n",
    "            \n",
    "            #tweet\n",
    "            for t in range(len(tweet_content)) : \n",
    "\n",
    "                tweet_id.append(tweet_content[t]['id'])\n",
    "                if 'geo' in tweet_content[t].keys():\n",
    "                    location_id_t.append(tweet_content[t]['geo']['place_id'] )\n",
    "                else:\n",
    "                    location_id_t.append('No geotag')\n",
    "                \n",
    "                text.append(tweet_content[t]['text'] )\n",
    "                \n",
    "                                \n",
    "            #location\n",
    "            if 'places' in tweets[i]['includes'].keys():\n",
    "                tweet_location = tweets[i]['includes']['places']\n",
    "                for l in range(len(tweet_location)):\n",
    "                    location_id_l.append(tweet_location[l]['id'])\n",
    "                    country.append(tweet_location[l]['country'])\n",
    "                    place_type.append(tweet_location[l]['place_type'])\n",
    "                    name_l.append(tweet_location[l]['name'])\n",
    "               \n",
    "       #Create the DataFrames\n",
    "        tweet_text_df = pd.DataFrame({\"tweet_id\":tweet_id,\n",
    "                                \"location_id\":location_id_t,\n",
    "                                \"text\":text})\n",
    "\n",
    "        location_df = pd.DataFrame({\"location_id\":location_id_l,\n",
    "                                \"country\":country,\n",
    "                                \"place_type\":place_type,\n",
    "                                \"location_geo\": name_l})\n",
    "\n",
    "        \n",
    "         #Remove all duplicates\n",
    "        location_df = location_df.drop_duplicates(subset= ['location_id'],\n",
    "                                                      keep='first', inplace=False, ignore_index=False)\n",
    "        #Merge the information from tweet and location dataframes\n",
    "        tweet_text_df = tweet_text_df.merge(location_df,on='location_id',how='left')\n",
    "        return tweet_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Start Date:  2017-01-01T00:00:00.000Z\n",
      "Total # of Tweets added:  5\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Start Date:  2017-02-01T00:00:00.000Z\n",
      "Total # of Tweets added:  1\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Start Date:  2017-04-01T00:00:00.000Z\n",
      "Total # of Tweets added:  1\n",
      "-------------------\n",
      "-------------------\n",
      "Token:  None\n",
      "Endpoint Response Code: 200\n",
      "Start Date:  2017-05-01T00:00:00.000Z\n",
      "Total # of Tweets added:  2\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "dl = DataLoaderSimpleVersion(bearer_token)   \n",
    "\n",
    "#Retrieve around 200 tweets, 100 on 2020/01/01 and 100 on 2020/01/02, written in English\n",
    "tweet_list = dl.retrieve_tweet(start_list=['2017-01-01T00:00:00.000Z','2017-02-01T00:00:00.000Z',\n",
    "                                          '2017-03-01T00:00:00.000Z','2017-04-01T00:00:00.000Z',\n",
    "                                          '2017-05-01T00:00:00.000Z','2017-06-01T00:00:00.000Z',\n",
    "                                          '2017-07-01T00:00:00.000Z','2017-08-01T00:00:00.000Z',\n",
    "                                          '2017-09-01T00:00:00.000Z','2017-10-01T00:00:00.000Z',\n",
    "                                          '2017-11-01T00:00:00.000Z','2017-12-01T00:00:00.000Z'],\n",
    "                               end_list=['2017-01-31T00:00:00.000Z','2017-02-28T00:00:00.000Z',\n",
    "                                        '2017-03-31T00:00:00.000Z','2017-04-30T00:00:00.000Z',\n",
    "                                        '2017-05-31T00:00:00.000Z','2017-06-30T00:00:00.000Z',\n",
    "                                        '2017-07-31T00:00:00.000Z','2017-08-31T00:00:00.000Z',\n",
    "                                        '2017-09-30T00:00:00.000Z','2017-10-31T00:00:00.000Z',\n",
    "                                        '2017-11-30T00:00:00.000Z','2017-12-31T00:00:00.000Z'],\n",
    "                                keyword=\"mindfulness place_country:BE has:geo lang:nl\",\n",
    "                               tweet_per_period=100, \n",
    "                               )\n",
    "                               \n",
    "#Convert the Twitter json output to csv  and save files \n",
    "\n",
    "tweet = dl.to_dataframe(tweet_list)\n",
    "tweet.to_csv('tweet_info2017mindfulness.csv',sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742e6c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
