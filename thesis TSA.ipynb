{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87596343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21cf466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search API allows access to tweets from previous week\n",
    "\n",
    "#streaming API: real-time tweets:\n",
    "#for data collection filer endpoint is used! In this way you are able to add keywords, userIDs and locations to search\n",
    "#sample endpoint: random sample\n",
    "\n",
    "#we use streaming API, filter endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a106aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "consumer_key= 7QYx97CYLi5FGgfqdtqVF9hMq\n",
    "consumer_secret= oKsnPVjyMB7t41F485ykygIARdK83pPnttfJVsTeFCO7w5xinz\n",
    "acces_token= 1444958072779288590-Vy9PjHzfNeN16u8McWIJsGfk2LxQhN\n",
    "acces_token_secret = 6s6GdikBh8LCC40YLyjrO6C5HVV5yy5kqJZKASRlZb1s4\n",
    "auth= 0AuthHanddler(consumer_key, consumer_secret)\n",
    "auth.set_acces_token(acces_token,acces_token_secret)\n",
    "api=API(auth)\n",
    "\n",
    "from tweepy import Stream\n",
    "keywords_to_track = ['#rstats', '#python']\n",
    "listen = Slistener(api)\n",
    "stream = Stream(auth,listen)\n",
    "stream.filter(track = keywords_to_track)\n",
    "\n",
    "#convert the twitter json output to csv and save files\n",
    "tweet, user= dl.to_dataframe(tweet_list)\n",
    "tweet.to_csv('output_path')\n",
    "user.to_csv('output_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data collection when sample endpoint is used (needed for demographic inference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_collection import Dataloader\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAHXvUQEAAAAAPQ%2FXl8vv2ZxuPlNEMn%2BbAvOg428%3DCNmiqFmvchzJIzzhLvyFBvL1mnOuwGjALg6uSbECpZIojXxUIM\"\n",
    "dl = DataLoader(bearer_token)\n",
    "\n",
    "#retrieve 100 tweets, 50 on 20/10/2022 and 50 on 21/10/2022 \n",
    "tweet_list= dl.retrieve_tweet(start_list=['2022-10-20T00:00:00.000Z','2022-10-20T00:00:00.000Z'],\n",
    "                               end_list=['2022-10-21T00:00:00.000Z','2022-10-21T00:00:00.000Z'],\n",
    "                               keyword=\"lang:en\",\n",
    "                               tweet_per_period=50\n",
    "                               )\n",
    "#convert the twitter json output to csv and save files\n",
    "tweet, user= dl.to_dataframe(tweet_list)\n",
    "tweet.to_csv('output_path')\n",
    "user.to_csv('output_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing JSON: in order to print tweet text, ID, user handle, follower count, user location, user description\n",
    "tweet_json= open('tweet-example.json', 'r').read()\n",
    "tweet= json.loads(tweet_json)\n",
    "tweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to analyse tweets in scale we need a pandas dictionary, herefore flattening required\n",
    "tweets_list=[]\n",
    "with open('all_tweets.json, 'r') as fh:\n",
    "          tweets_json= fh.read().split(\"\\n\")\n",
    "          for tweet in tweets_json:\n",
    "          tweet_obj = json.loads(tweet)\n",
    "          if 'extended_tweet' in tweet_obj:\n",
    "              tweet_obj['extended_tweet-full_text']=tweet_obj['extended_tweet']['full_text']\n",
    "          \n",
    "          tweet_list.append(tweet)\n",
    "          tweets= pd.DataFrame(tweet_list)\n",
    "          print(ds_tweets['text'].value[0:5])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis, neutral: measures words that don't contribute to the sentiment, compound: combination of positive and negative: above 0 = positive, below 0 = negative\n",
    "from nltk.sentiment.vader impirt SentimentIntensityAnalyzer\n",
    "sid= SentimentIntensityAnalyzer()\n",
    "sentiment_scores= tweets['text'].apply(sid.polarity_scores)\n",
    "# Generate average sentiment scores for #python\n",
    "sentiment_py = sentiment[ check_word_in_tweet('#python', ds_tweets) ].resample('1 d').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8565b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting sentiment scores\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average #python sentiment per day\n",
    "plt.plot(sentiment_py.index.day, sentiment_py, color = 'green')\n",
    "\n",
    "# Plot average #rstats sentiment per day\n",
    "plt.plot(sentiment_r.index.day, sentiment_r, color = 'blue')\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Sentiment of data science languages')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo-location\n",
    "#NLP can look for the mention of a location in the tweet and that the user is at this location: hard to do\n",
    "#user-defined location field\n",
    "print(tweet['user']['location'])\n",
    "#place is child json where we can find bounding box: longitude and latitude coordinates\n",
    "#translate bounding box to a centroid: midpoint of boundingbox\n",
    "\n",
    "# Print out the location of a single tweet\n",
    "print(tweet_json['user']['location'])\n",
    "\n",
    "# Flatten and load the SOTU tweets into a dataframe\n",
    "tweets_sotu = pd.DataFrame(flatten_tweets(tweets_sotu_json))\n",
    "\n",
    "# Print out top five user-defined locations\n",
    "print(tweets_sotu['user-location'].value_counts().head())\n",
    "\n",
    "def getBoundingBox(place):\n",
    "    \"\"\" Returns the bounding box coordinates.\"\"\"\n",
    "    return place['bounding_box']['coordinates']\n",
    "\n",
    "# Apply the function which gets bounding box coordinates\n",
    "bounding_boxes = tweets_sotu['place'].apply(getBoundingBox)\n",
    "\n",
    "# Print out the first bounding box coordinates\n",
    "print(bounding_boxes.values[0])\n",
    "\n",
    "def calculateCentroid(place):\n",
    "    \"\"\" Calculates the centroid from a bounding box.\"\"\"\n",
    "    # Obtain the coordinates from the bounding box.\n",
    "    coordinates = place['bounding_box']['coordinates'][0]\n",
    "    \n",
    "    longs = np.unique( [x[0] for x in coordinates] )\n",
    "    lats  = np.unique( [x[1] for x in coordinates] )\n",
    "    \n",
    "    if len(longs) == 1 and len(lats) == 1:\n",
    "        # return a single coordinate\n",
    "        return (longs[0], lats[0])\n",
    "    elif len(longs) == 2 and len(lats) == 2:\n",
    "        # If we have two longs and lats, we have a box.\n",
    "        central_long = np.sum(longs) / 2\n",
    "        central_lat  = np.sum(lats) / 2\n",
    "    else:\n",
    "        raise ValueError(\"Non-rectangular polygon not supported.\")\n",
    "\n",
    "    return (central_long, central_lat)\n",
    "\n",
    "# Calculate the centroids of place     \n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
